{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "tzifpavdqyqewux94undas",
    "execution_id": "830e5068-e231-4b20-9030-53452dcafbb9"
   },
   "source": [
    "Colab related:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "cellId": "twa2b910vetdy3imvw9ke"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# !cp -r \"/content/drive/MyDrive/Colab Notebooks/Diploma/handle_amazon/amazon_en\" .\n",
    "# !cp -r \"/content/drive/MyDrive/Colab Notebooks/Diploma/handle_amazon/amazon_fr\" .\n",
    "# !cp -r \"/content/drive/MyDrive/Colab Notebooks/Diploma/handle_amazon/amazon_de\" .\n",
    "# !cp -r \"/content/drive/MyDrive/Colab Notebooks/Diploma/handle_amazon/amazon_es\" .\n",
    "\n",
    "# !pip install transformers datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "v831r2aygog3x9slcprgqp",
    "execution_id": "660717de-ca1c-4d89-a9c2-74dcfc889650"
   },
   "source": [
    "DS related:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "cellId": "6l4nyns1vqw0te31t4kcxiq"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "# %pip install seaborn\n",
    "# %pip install transformers datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "rxhcw6use2ebj2mdo2000h",
    "execution_id": "b4487512-d86e-48ac-8547-78551a5551bd"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "cellId": "fxdwij12bpolkqq4qile0p"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def nice_df(df, axis=None, reverse=False, **kwargs):\n",
    "    cm = sns.light_palette(\"green\", as_cmap=True, reverse=reverse)\n",
    "    return df.style.background_gradient(cmap=cm, axis=axis, **kwargs)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "feqye9ul8wpeaqq4kq58fh",
    "execution_id": "a96449b7-f86e-475a-9cf1-fd3ba0db91ea"
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "cellId": "xtgnwed3pic2eib69a5kdo"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "# !unzip handle_amazon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "cellId": "u48gfhuc15rxodqn8e2oh"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "from datasets import concatenate_datasets, load_from_disk\n",
    "\n",
    "BS = 32\n",
    "lang_list = ['en', 'fr', 'de', 'es']\n",
    "split_list = ['train', 'validation', 'test']\n",
    "\n",
    "\n",
    "# data = {\n",
    "#     lang: load_from_disk(f'handle_amazon/amazon_{lang}')\n",
    "#     for lang in lang_list\n",
    "# }\n",
    "\n",
    "tr_data = {\n",
    "    lang: load_from_disk(f'handle_amazon/amazon_ok_tr_{lang}')\n",
    "    for lang in lang_list\n",
    "}\n",
    "\n",
    "dataloader = {\n",
    "    lang: {\n",
    "        split: DataLoader(tr_data[lang][split], batch_size=BS, shuffle=(split == 'train'))\n",
    "        for split in split_list\n",
    "    }\n",
    "    for lang in lang_list\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "l7nvel55c7kc331er7atci",
    "execution_id": "28c44492-3c8d-41ee-a150-d0de2d873633"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "cellId": "dvhg8jfj2rmlm4lcit0ekd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3267,  0.0176,  0.1214,  ...,  0.0836, -0.0200, -0.1360],\n",
      "        [ 0.1976,  0.1497,  0.1829,  ...,  0.0148,  0.0260, -0.0101],\n",
      "        [ 0.1492,  0.0464,  0.1645,  ..., -0.0861, -0.0884,  0.0062],\n",
      "        ...,\n",
      "        [ 0.1911,  0.1054, -0.0288,  ..., -0.0056,  0.0567, -0.1306],\n",
      "        [ 0.1369,  0.0744,  0.1030,  ...,  0.0234, -0.1261, -0.0686],\n",
      "        [ 0.1460,  0.2451,  0.0183,  ...,  0.1160, -0.0157, -0.1809]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0882, -0.0922],\n",
      "        [ 0.0982, -0.0514],\n",
      "        [ 0.0579, -0.0992],\n",
      "        [ 0.0792, -0.0381],\n",
      "        [ 0.0556, -0.1030],\n",
      "        [ 0.0936, -0.0238],\n",
      "        [ 0.0797, -0.0974],\n",
      "        [ 0.1063, -0.0777],\n",
      "        [ 0.0753, -0.0461],\n",
      "        [ 0.0592, -0.0841],\n",
      "        [ 0.1030, -0.0897],\n",
      "        [ 0.1133, -0.0407],\n",
      "        [ 0.1020, -0.0723],\n",
      "        [ 0.1054, -0.0458],\n",
      "        [ 0.0810, -0.0876],\n",
      "        [ 0.0785, -0.0557],\n",
      "        [ 0.0407, -0.0787],\n",
      "        [ 0.1149, -0.0726],\n",
      "        [ 0.0706, -0.0392],\n",
      "        [ 0.0449, -0.0648],\n",
      "        [ 0.0306, -0.1089],\n",
      "        [ 0.0698, -0.0846],\n",
      "        [ 0.1122, -0.1054],\n",
      "        [ 0.0912, -0.0733],\n",
      "        [ 0.0901, -0.0470],\n",
      "        [ 0.0541, -0.0889],\n",
      "        [ 0.0338, -0.0719],\n",
      "        [ 0.0988, -0.0816],\n",
      "        [ 0.0389, -0.0561],\n",
      "        [ 0.0763, -0.0855],\n",
      "        [ 0.1144, -0.1171],\n",
      "        [ 0.0796, -0.0961]], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "\n",
    "models = dict()\n",
    "for lang in lang_list:\n",
    "    models[lang] = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id, output_hidden_states=True)\n",
    "    models[lang].to(device)\n",
    "#     for param in models[lang].base_model.parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dataloader['en']['test']:\n",
    "        i_d = batch[\"input_ids\"].to(device)\n",
    "        a_m = batch[\"attention_mask\"].to(device)\n",
    "        batch_hs = models['en'](\n",
    "                input_ids=i_d,\n",
    "                attention_mask=a_m,\n",
    "            ).hidden_states[-1].mean(dim=1)    \n",
    "        print(batch_hs)\n",
    "\n",
    "        logits = models['en'](\n",
    "                input_ids=i_d,\n",
    "                attention_mask=a_m,\n",
    "            ).logits    \n",
    "        print(logits)\n",
    "\n",
    "        print(torch.argmax(\n",
    "            models['en'](\n",
    "                input_ids=i_d,\n",
    "                attention_mask=a_m,\n",
    "            ).logits,\n",
    "            axis=-1\n",
    "        ))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "cellId": "iqyx4dgc9ssvmbltew77l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_classifier.weight torch.Size([768, 768]) True\n",
      "pre_classifier.bias torch.Size([768]) True\n",
      "classifier.weight torch.Size([2, 768]) True\n",
      "classifier.bias torch.Size([2]) True\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "for name, param in models['en'].named_parameters():\n",
    "    if 'clas' in name:\n",
    "        print(name, param.shape, param.requires_grad)\n",
    "        assert param.requires_grad\n",
    "    else:\n",
    "        assert param.requires_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "2tx9ggh41g2u7nnvdtxwyq",
    "execution_id": "b0c6c125-e90a-4f5b-ade1-bdf0c9e05833"
   },
   "source": [
    "## Eval and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "cellId": "avhaedat3cbxjqz4secpwh"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def eval(model, dls, lang, test_split):\n",
    "    # put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # get needful data slice\n",
    "    dl_to_test = dls[lang][test_split]\n",
    "    \n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dl_to_test):\n",
    "            # move batch to device\n",
    "            input_ids = batch['input_ids'].to(model.device)\n",
    "            attention_mask = batch['attention_mask'].to(model.device)\n",
    "            labels = batch['bin_label'].to(model.device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # calculate loss and accuracy\n",
    "            preds = logits.argmax(dim=1)\n",
    "            test_acc += (preds == labels).sum().item()\n",
    "\n",
    "    test_acc /= BS * len(dl_to_test)\n",
    "    print(f'\\teval {lang}: {test_acc}')\n",
    "    return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "cellId": "84x8yyc3nemue9h5ppg4ob"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def train(model, dls, lang, train_split, validation_split, num_epochs=2, device='mps'):\n",
    "    # put model on mps device\n",
    "    model.to(device)\n",
    "    \n",
    "    # get needful data slice\n",
    "    dl_to_train = dls[lang][train_split]\n",
    "    dl_to_valid = dls[lang][validation_split]\n",
    "\n",
    "    # define our optimizer and loss function\n",
    "    learning_rate_bert = 1e-7\n",
    "    learning_rate_classifier = 2e-5\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": model.distilbert.parameters(), \"lr\": learning_rate_bert},\n",
    "        {\"params\": model.classifier.parameters(), \"lr\": learning_rate_classifier},\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # train loop\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        for batch in tqdm(dl_to_train):\n",
    "            # move batch to device\n",
    "            input_ids = batch['input_ids'].to(model.device)\n",
    "            attention_mask = batch['attention_mask'].to(model.device)\n",
    "            labels = batch['bin_label'].to(model.device)\n",
    "\n",
    "            # zero out gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # calculate loss and accuracy\n",
    "            loss = loss_fn(logits, labels)\n",
    "            train_loss += loss.item()\n",
    "            preds = logits.argmax(dim=1)\n",
    "            train_acc += (preds == labels).sum().item()\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "        train_acc /= BS * len(dl_to_train)\n",
    "        valid_acc = eval(model, dls, lang, validation_split)\n",
    "        print(f'train {lang}: {train_acc} (val {valid_acc})')\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "yrd3fhstfjgijz0jbsxaw9",
    "execution_id": "72365522-e30e-49cf-a7f2-833935288d12"
   },
   "source": [
    "## All Lang Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "cellId": "8c7swb5tp9bgz938gqeq6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\teval en: 0.8765\n",
      "train en: 0.829375 (val 0.8765)\n",
      "\teval en: 0.88625\n",
      "train en: 0.88894375 (val 0.88625)\n",
      "\teval fr: 0.7655\n",
      "train fr: 0.67853125 (val 0.7655)\n",
      "\teval fr: 0.81225\n",
      "train fr: 0.782325 (val 0.81225)\n",
      "\teval de: 0.73175\n",
      "train de: 0.6233375 (val 0.73175)\n",
      "\teval de: 0.776\n",
      "train de: 0.7256875 (val 0.776)\n",
      "\teval es: 0.75325\n",
      "train es: 0.6439375 (val 0.75325)\n",
      "\teval es: 0.78875\n",
      "train es: 0.74749375 (val 0.78875)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [42:33<00:00,  1.96it/s]\n",
      "100%|██████████| 125/125 [00:22<00:00,  5.52it/s]\n",
      "100%|██████████| 5000/5000 [42:33<00:00,  1.96it/s]\n",
      "100%|██████████| 125/125 [00:22<00:00,  5.50it/s]\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "for lang in lang_list:\n",
    "    train(models[lang], dataloader, lang, 'train', 'validation', num_epochs=2, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "cellId": "c2a6zdgk8iqi1qw2gd1mkj"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "\n",
    "# for lang in lang_list:\n",
    "#     if lang != 'en':\n",
    "#         train(models[lang], dataloader, lang, 'train', 'validation', lang=lang, num_epochs=3, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "cellId": "mxlcro8et1n13c8ocxff2c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:22<00:00,  5.50it/s]\n",
      "100%|██████████| 125/125 [00:22<00:00,  5.51it/s]\n",
      "100%|██████████| 125/125 [00:22<00:00,  5.50it/s]\n",
      "100%|██████████| 125/125 [00:22<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\teval en: 0.901\n",
      "\teval fr: 0.813\n",
      "\teval de: 0.775\n",
      "\teval es: 0.799\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_b966ce52_0353_11ee_8d5d_d00d91c12750row0_col0 {\n",
       "            background-color:  #008000;\n",
       "            color:  #f1f1f1;\n",
       "        }    #T_b966ce52_0353_11ee_8d5d_d00d91c12750row1_col0 {\n",
       "            background-color:  #a4d0a4;\n",
       "            color:  #000000;\n",
       "        }    #T_b966ce52_0353_11ee_8d5d_d00d91c12750row2_col0 {\n",
       "            background-color:  #ebf3eb;\n",
       "            color:  #000000;\n",
       "        }    #T_b966ce52_0353_11ee_8d5d_d00d91c12750row3_col0 {\n",
       "            background-color:  #beddbe;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_b966ce52_0353_11ee_8d5d_d00d91c12750\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >no_freeze</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_b966ce52_0353_11ee_8d5d_d00d91c12750level0_row0\" class=\"row_heading level0 row0\" >en</th>\n",
       "                        <td id=\"T_b966ce52_0353_11ee_8d5d_d00d91c12750row0_col0\" class=\"data row0 col0\" >0.901</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b966ce52_0353_11ee_8d5d_d00d91c12750level0_row1\" class=\"row_heading level0 row1\" >fr</th>\n",
       "                        <td id=\"T_b966ce52_0353_11ee_8d5d_d00d91c12750row1_col0\" class=\"data row1 col0\" >0.813</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b966ce52_0353_11ee_8d5d_d00d91c12750level0_row2\" class=\"row_heading level0 row2\" >de</th>\n",
       "                        <td id=\"T_b966ce52_0353_11ee_8d5d_d00d91c12750row2_col0\" class=\"data row2 col0\" >0.775</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_b966ce52_0353_11ee_8d5d_d00d91c12750level0_row3\" class=\"row_heading level0 row3\" >es</th>\n",
       "                        <td id=\"T_b966ce52_0353_11ee_8d5d_d00d91c12750row3_col0\" class=\"data row3 col0\" >0.799</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fa1b6c40250>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!g1.1\n",
    "eval_res = pd.DataFrame(data = np.zeros((4, 1)), columns = ['no_freeze'], index=lang_list)\n",
    "\n",
    "for lang in lang_list:\n",
    "    test_res = eval(models[lang], dataloader, lang, 'test')\n",
    "    eval_res.at[lang, 'no_freeze'] = test_res\n",
    "\n",
    "nice_df(eval_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "cellId": "yrlxz4ie5zz3e0d9r2ap"
   },
   "outputs": [],
   "source": [
    "del_datasphere_variables('id2label', 'label2id', 'models', 'param')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "cellId": "m50m7r68ni7n74xnwgeia"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ada2f562badd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'models/ft_full_en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "models['en'].save_pretrained(f'models/ft_full_en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "cellId": "ayqg6di0kxpi3yxynhtkr"
   },
   "outputs": [],
   "source": [
    "# #!g1.1\n",
    "# for lang in lang_list:\n",
    "#     models[lang].save_pretrained(f'models/ft_no_tr_{lang}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "cellId": "tr8pod1puznnh3sukqor"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n",
      "5000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "#!g1.1\n",
    "for lang in lang_list:\n",
    "    print(len(dataloader[lang]['train']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "mziqfsye3qfxlfhsvr9zhs"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "317b92a1-c60c-426e-845d-349247fb00b1",
  "notebookPath": "finetune_no_freeze.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
